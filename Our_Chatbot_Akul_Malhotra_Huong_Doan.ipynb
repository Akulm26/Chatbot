{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Our Chatbot_Akul Malhotra_Huong Doan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5gRABMgeLNS"
      },
      "source": [
        "Team members:\n",
        "* Akul Malhotra\n",
        "* Huong Doan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsRvUU-q7grc"
      },
      "source": [
        "This project is following the instruction of the article: https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-d411c8738ab5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9pGvZI36dE8"
      },
      "source": [
        "# Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSehpNAi0jpn",
        "outputId": "5193004a-028f-4511-83ab-d9eee67d6ef9"
      },
      "source": [
        "# clear defined variables\n",
        "#%reset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiCx05sh6cbY"
      },
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBHLB3Sl6po4"
      },
      "source": [
        "# Load dataset\n",
        "\n",
        "Dataset https://www.kaggle.com/grafstor/simple-dialogs-for-chatbot?select=dialogs.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR__AE8E6pzI",
        "outputId": "8c1ed41f-b0d5-4797-c845-f7dcac14b7a5"
      },
      "source": [
        "# Defining lines as a list of each line\n",
        "with open(\"dialogs.txt\", 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "print(len(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyKKgQg27DC5",
        "outputId": "68daac00-cf9a-49cb-feab-c3329c7edaa3"
      },
      "source": [
        "lines[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"hi, how are you doing?\\ti'm fine. how about yourself?\",\n",
              " \"i'm fine. how about yourself?\\ti'm pretty good. thanks for asking.\",\n",
              " \"i'm pretty good. thanks for asking.\\tno problem. so how have you been?\",\n",
              " \"no problem. so how have you been?\\ti've been great. what about you?\",\n",
              " \"i've been great. what about you?\\ti've been good. i'm in school right now.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvf3sL1U6p3K",
        "outputId": "3981e408-e1e7-48f7-ea9a-2c2438db9f04"
      },
      "source": [
        "lines[0].split(sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi, how are you doing?', \"i'm fine. how about yourself?\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmm1rel_EqRI"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycZDZn8jBykr",
        "outputId": "450a55db-b9b2-408a-d96c-ce11c465029b"
      },
      "source": [
        "# Seperate input and reply lines\n",
        "input_lines = []\n",
        "reply_lines = []\n",
        "\n",
        "for line in lines:\n",
        "  input_text, reply_text = line.split(sep=\"\\t\")\n",
        "  input_lines.append(input_text)\n",
        "  reply_lines.append(reply_text)\n",
        "\n",
        "print(len(input_lines))\n",
        "print(len(reply_lines))\n",
        "print(input_lines[0:20])\n",
        "print(reply_lines[0:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3725\n",
            "3725\n",
            "['hi, how are you doing?', \"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?', \"i've been great. what about you?\", \"i've been good. i'm in school right now.\", 'what school do you go to?', 'i go to pcc.', 'do you like it there?', \"it's okay. it's a really big campus.\", 'good luck with school.', \"how's it going?\", \"i'm doing well. how about you?\", 'never better, thanks.', 'so how have you been lately?', \"i've actually been pretty good. you?\", \"i'm actually in school right now.\", 'which school do you attend?', \"i'm attending pcc right now.\", 'are you enjoying it there?']\n",
            "[\"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?', \"i've been great. what about you?\", \"i've been good. i'm in school right now.\", 'what school do you go to?', 'i go to pcc.', 'do you like it there?', \"it's okay. it's a really big campus.\", 'good luck with school.', 'thank you very much.', \"i'm doing well. how about you?\", 'never better, thanks.', 'so how have you been lately?', \"i've actually been pretty good. you?\", \"i'm actually in school right now.\", 'which school do you attend?', \"i'm attending pcc right now.\", 'are you enjoying it there?', \"it's not bad. there are a lot of people there.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofS5P_9svzzn",
        "outputId": "fe2e925d-4c81-4fee-9c48-b3d4d16ebb35"
      },
      "source": [
        "# grouping lines by response pair\n",
        "pairs = list(zip(input_lines, reply_lines))\n",
        "print(len(pairs))\n",
        "pairs[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hi, how are you doing?', \"i'm fine. how about yourself?\"),\n",
              " (\"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\"),\n",
              " (\"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?'),\n",
              " ('no problem. so how have you been?', \"i've been great. what about you?\"),\n",
              " (\"i've been great. what about you?\",\n",
              "  \"i've been good. i'm in school right now.\"),\n",
              " (\"i've been good. i'm in school right now.\", 'what school do you go to?'),\n",
              " ('what school do you go to?', 'i go to pcc.'),\n",
              " ('i go to pcc.', 'do you like it there?'),\n",
              " ('do you like it there?', \"it's okay. it's a really big campus.\"),\n",
              " (\"it's okay. it's a really big campus.\", 'good luck with school.'),\n",
              " ('good luck with school.', 'thank you very much.'),\n",
              " (\"how's it going?\", \"i'm doing well. how about you?\"),\n",
              " (\"i'm doing well. how about you?\", 'never better, thanks.'),\n",
              " ('never better, thanks.', 'so how have you been lately?'),\n",
              " ('so how have you been lately?', \"i've actually been pretty good. you?\"),\n",
              " (\"i've actually been pretty good. you?\", \"i'm actually in school right now.\"),\n",
              " (\"i'm actually in school right now.\", 'which school do you attend?'),\n",
              " ('which school do you attend?', \"i'm attending pcc right now.\"),\n",
              " (\"i'm attending pcc right now.\", 'are you enjoying it there?'),\n",
              " ('are you enjoying it there?',\n",
              "  \"it's not bad. there are a lot of people there.\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VJV-TxDv_Ro",
        "outputId": "01957f08-8581-4bb6-ef86-6b48527a1859"
      },
      "source": [
        "# Shuffle the pairs \n",
        "random.shuffle(pairs)\n",
        "print(len(pairs))\n",
        "pairs[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the forecast says that it will be warm on the weekend.',\n",
              "  \"so do you think it'll be perfect weather for the beach?\"),\n",
              " ('yes, those eight years were a lot of fun for everyone.',\n",
              "  'only 4,000 american soldiers were killed overseas.'),\n",
              " ('what happened?',\n",
              "  'i gave her $1,000 for her birthday. i told her to spend it on herself.'),\n",
              " ('yeah, i went. did you go?', \"no, i didn't feel like it.\"),\n",
              " ('i hope i win the lotto.', 'your chances are very small.'),\n",
              " ('a good story is more important than color.',\n",
              "  \"actors didn't curse back then.\"),\n",
              " (\"then they're worth every penny.\", 'you might want to buy a pair.'),\n",
              " ('me, too. school was fun.', 'and it was only 12 years.'),\n",
              " ('i have to go to the bathroom.', 'you drink too much coffee.'),\n",
              " ('and you get a lot of exercise every day.', \"that's the truth.\"),\n",
              " ('are you sure?', 'we will be house rich, but cash poor.'),\n",
              " ('next time you go to the market, let me go with you.',\n",
              "  'no, thank you. all you want to eat are hot dogs and candy bars.'),\n",
              " (\"we get a lot of things from cows, don't we?\",\n",
              "  \"yes. a cow is man's best friend.\"),\n",
              " ('we need cheese, bread, and ham.', 'what kind of cheese?'),\n",
              " (\"of course. it's not a hard job.\", \"i'll help you.\"),\n",
              " ('every easter sunday he gives away money.', 'is it his money?'),\n",
              " ('you would do the same for me.', 'of course. what are friends for?'),\n",
              " (\"it's worse than that.\", 'how so?'),\n",
              " (\"what's the matter?\", 'i was on a plane.'),\n",
              " ('yes. after two u.s. fighter jets followed him for an hour, he landed on a highway.',\n",
              "  'did he crash?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgexEamjGFeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb0f4dc-771c-41dd-c389-42b116787c4b"
      },
      "source": [
        "# For target sequences, we will add ‘<START>’ at the beginning of the sequence and ‘<END>’ \n",
        "#    at the end of the sequence so that our model knows where to start and end text generation. \n",
        "input_lines_2 = []\n",
        "reply_lines_2 = []\n",
        "input_tokens = set()\n",
        "reply_tokens = set()\n",
        "\n",
        "for p in pairs:\n",
        "  input_text, reply_text = p[0], p[1]\n",
        "  \n",
        "  # remove non-alphabet\n",
        "  input_text = re.sub(\"[^a-zA-Z]\", \" \", input_text)\n",
        "  reply_text = re.sub(\"[^a-zA-Z]\", \" \", reply_text)\n",
        "  reply_text = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", reply_text))\n",
        "  reply_text = '<START> ' + reply_text + ' <END>'\n",
        "\n",
        "  input_lines_2.append(input_text)\n",
        "  #reply_lines_2.append(reply)\n",
        "  reply_lines_2.append(reply_text)\n",
        " \n",
        "  # tokenize the input and reply lines\n",
        "  for token in input_text.split(): #re.findall(r\"[\\w']+|[^\\s\\w]\", input_text):\n",
        "    input_tokens.add(token)\n",
        "  for token in reply_text.split(): #re.findall(r\"[\\w'<>]+|[^\\s\\w]\", reply_text):\n",
        "    reply_tokens.add(token)\n",
        "\n",
        "\n",
        "print(input_tokens)\n",
        "print(reply_tokens)\n",
        "\n",
        "input_tokens = sorted(list(input_tokens))\n",
        "reply_tokens = sorted(list(reply_tokens))\n",
        "\n",
        "print(len(input_tokens))\n",
        "print(len(reply_tokens))\n",
        "print(input_tokens[:20])\n",
        "print(reply_tokens[:20])\n",
        "\n",
        "print(len(input_lines_2))\n",
        "print(len(reply_lines_2))\n",
        "print(input_lines_2[:20])\n",
        "print(reply_lines_2[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'forecast', 'm', 'cut', 'channels', 'flight', 'ashes', 'speech', 'cartoons', 'address', 'yet', 'for', 'speeding', 'unbelievable', 'jar', 'am', 'medication', 'stole', 'license', 'rude', 'golfers', 'folded', 'rules', 'deserved', 'housekeeping', 'chop', 'explain', 'tears', 'couldn', 'floors', 'both', 'long', 'colder', 'sitting', 'fingers', 'extra', 'various', 'things', 'noticed', 'snow', 'two', 'unit', 'insert', 'atlantic', 'crying', 'falling', 'christmas', 'stopping', 'suction', 'adding', 'rich', 'school', 'quite', 'god', 'navel', 'plus', 'news', 'wipes', 'reuse', 'stops', 'jacket', 'handyman', 'court', 'savings', 'hitting', 'students', 'survive', 'nation', 'artist', 'animal', 'feelings', 'squeeze', 'sliced', 'guarantees', 'rose', 'at', 'rush', 'prices', 'smoker', 'butterflies', 'learned', 'crowded', 'will', 'added', 'uses', 'standing', 'somewhere', 'potato', 'vacuumed', 'siren', 'degrees', 'poodle', 'overseas', 'crosswalk', 'under', 'seeing', 'worst', 'killer', 'shows', 'cart', 'licks', 'yeah', 'parties', 'visited', 'star', 'hopefully', 'earth', 'aid', 'always', 'ruined', 'son', 'wearing', 'sandwich', 'automatic', 'mall', 'stand', 'animals', 'twenty', 'reservation', 'hitter', 'question', 'blade', 'gently', 'takes', 'loves', 'drew', 'crazy', 'barbara', 'that', 'deli', 'rings', 'hands', 'pepto', 'damp', 'smell', 'guys', 'window', 'ever', 'pets', 'voted', 'starbucks', 'tasted', 'laughing', 'gravity', 'watch', 'cross', 'went', 'sprinkled', 'green', 'favor', 'ebay', 'same', 'drink', 'wood', 'add', 'artists', 'away', 'cook', 'parked', 'nice', 'band', 'brought', 'vacuum', 'outer', 'celery', 'lot', 'snowman', 'lawns', 'voters', 'dry', 'clearly', 'subscribed', 'through', 'apartment', 'waistband', 'boil', 'tape', 'live', 'class', 'coffin', 'genes', 'many', 'either', 'cigarette', 'another', 'shoe', 'reporters', 'plans', 'trust', 'fell', 'flower', 'calling', 'outside', 'honor', 'slow', 'hungry', 'stood', 'thin', 'alley', 'sew', 'hell', 'about', 'asking', 'jammed', 'market', 'lasts', 'rosters', 'rub', 'flat', 'prefer', 'a', 'sleeves', 'good', 'since', 'minute', 'catch', 'brushing', 'angry', 'rice', 'desktop', 'stomach', 'dd', 'matter', 'store', 'used', 'before', 'nails', 'starving', 'houses', 'teaching', 'seriously', 'sharp', 'eat', 'soup', 'wins', 'listening', 'towel', 'tutor', 'head', 'ha', 'invitation', 'buyer', 'knee', 'cares', 'china', 'had', 'gaining', 'carton', 'wear', 'titanic', 'damage', 'where', 'weather', 'thousand', 'pounds', 'stamp', 'having', 'stain', 'soap', 'joke', 'early', 'bush', 'silly', 'bowling', 'everything', 'mean', 'heaven', 'know', 'she', 'invitations', 'pocket', 'cold', 'other', 'world', 'working', 'down', 'hysterically', 'their', 'facial', 'reminder', 'army', 'nonsmoking', 'stop', 'time', 'knock', 'phone', 'kittens', 'hilarious', 'tires', 'marks', 'immediately', 'loosened', 'face', 'bed', 'goes', 'planning', 'wants', 'today', 'basketball', 'fly', 'controls', 'uncertain', 'dying', 'raw', 'fifteen', 'enough', 'almost', 'lots', 'waiter', 'unfriendly', 'practice', 'heat', 'helped', 'washington', 'lotto', 'machine', 'hurts', 'tax', 'party', 'by', 'made', 'birth', 'music', 'ears', 'man', 'room', 'glue', 'magazines', 'pages', 'everybody', 'pair', 'increase', 'sharpen', 'mine', 'french', 'times', 'computers', 'remind', 'herself', 'main', 'pizza', 'pictures', 'checked', 'cruise', 'lately', 'positive', 'cheaper', 'cause', 'entered', 'none', 'over', 'arm', 'lunch', 'ring', 'closer', 'blow', 'sunny', 'belong', 'attitude', 'unless', 'rate', 'plants', 'honestly', 'continued', 'unlucky', 'popular', 'isn', 'customer', 'textbooks', 'rhyme', 'taken', 'book', 'monthly', 'canyon', 'filthy', 'cry', 'supposed', 'sell', 'learning', 'victorious', 'credit', 'pockets', 'all', 'actor', 'anita', 'common', 'goats', 'plastic', 'ride', 'voting', 'slips', 'sticks', 'goodness', 'seem', 'sure', 'days', 'film', 'drinks', 'unsafe', 'drove', 'flown', 'can', 'certain', 'babies', 'gambling', 'iq', 'belt', 'people', 'ago', 'mattress', 'classroom', 'sort', 'surgery', 'dozen', 'later', 'course', 'bitter', 'withdraw', 'spanish', 'line', 'examine', 'ridiculous', 'afterwards', 'carts', 'sad', 'new', 'must', 'scrub', 'figured', 'wind', 'whatever', 'shop', 'sleeve', 'initial', 'breath', 'lotion', 'number', 'mustard', 'go', 'turned', 'deals', 'beach', 'teachers', 'fruits', 'everywhere', 'yours', 'section', 'unbuttoned', 'english', 'low', 'reheated', 'sometimes', 'promotion', 'carpet', 'bathroom', 'spit', 'vegetables', 'dad', 'earplugs', 'buys', 'vegas', 'freeways', 'walk', 'basically', 'eyes', 'whenever', 'hold', 'cone', 'showers', 'painter', 'weighs', 'mexico', 'boyfriend', 'cemetery', 'debit', 'funeral', 'salads', 'normal', 'guy', 'channel', 'winter', 'favorite', 'happens', 'lettuce', 'word', 'babysitter', 'prescription', 'tell', 'owned', 'invented', 'changed', 'dream', 'weatherman', 'milk', 'earthquakes', 'apologized', 'nearby', 'each', 'crash', 'tasty', 'steak', 'drugs', 'us', 'happening', 'what', 'seen', 'chain', 'blue', 'draw', 'faster', 'stove', 'heard', 'hp', 'score', 'swelling', 'closed', 'buried', 'dogs', 'tests', 'looked', 'harsh', 'wash', 'steps', 'up', 'bus', 'friday', 'sponge', 'honk', 'so', 'running', 'cow', 'ca', 'improve', 'no', 'corporations', 'or', 'boss', 'joining', 'breathe', 'stock', 'threat', 'reviews', 'sewing', 'retires', 'c', 'drowned', 'ice', 'lonely', 'finished', 'hundred', 'teeth', 'dressing', 'thirteenth', 'heated', 'guns', 'lawsuits', 'chased', 'kid', 'you', 'lead', 'happyness', 'points', 'shelf', 'causing', 'season', 'else', 'luck', 'taking', 'singer', 'foot', 'enjoyed', 'email', 'remember', 'racist', 'as', 'fingernails', 'passes', 'explode', 'raining', 'watching', 'dating', 'backpack', 'sides', 'democratic', 'afford', 'recently', 'moved', 'sister', 'table', 'sometime', 'tissues', 'homes', 'chemistry', 'major', 'nervous', 'amateur', 'floss', 'speaking', 'packs', 'math', 'scratching', 'movie', 'law', 'dangerous', 'concrete', 'men', 'pbs', 'speaker', 'fortune', 'checkout', 'mountains', 'alive', 'done', 'shells', 'play', 'gloves', 'stayed', 'honda', 'poetry', 'divorced', 'chance', 'sound', 'shine', 'needs', 'calculator', 'girl', 'daughter', 'front', 'bet', 'trip', 'candidate', 'forgot', 'chocolate', 'perfume', 'produce', 'wow', 'mccain', 'tiger', 'telephones', 'instead', 'voter', 'classmates', 'forgetting', 'possible', 'mud', 'death', 'sudden', 'aren', 'parts', 'dollars', 'feet', 'oops', 'arms', 'nobody', 'rubber', 'dent', 'outfit', 'dentist', 'pray', 'boring', 'o', 'fact', 'magazine', 'light', 'called', 'vote', 'poke', 'middle', 'wait', 'numbers', 'one', 'full', 'tomato', 'foul', 'sidewalks', 'still', 'relaxed', 'labor', 'thing', 'planet', 'easier', 'suspended', 'student', 'form', 'single', 'nap', 'work', 'toss', 'woke', 'tired', 'tv', 'have', 'burnt', 'raise', 'dark', 'shake', 'makes', 'prettiest', 'drawer', 'rods', 'traffic', 'to', 'killing', 'computer', 'rock', 'payments', 'troublemakers', 'laid', 'soft', 'b', 'offered', 'respect', 'riding', 'flossing', 'radio', 'fresh', 'say', 'antenna', 'met', 'back', 'complaining', 'finish', 'said', 'boobs', 'cigarettes', 'lungs', 'burner', 'upstairs', 'october', 'prove', 'smoking', 'city', 'badly', 'recycle', 'altitude', 'thank', 'quietly', 'meowing', 'works', 'with', 'values', 'stuff', 'write', 'chore', 'crossing', 'sleeping', 'part', 'church', 'meaning', 'saved', 'notebook', 'getting', 'leftovers', 'installation', 'let', 'pain', 'view', 'placed', 'paintings', 'pay', 'painted', 'pleasant', 'decided', 'diapers', 'unplug', 'stars', 'dried', 'injured', 'enjoy', 'talking', 'fireman', 'latest', 'even', 'f', 'theater', 'skills', 'hunch', 'happen', 'sing', 'spoke', 'kinds', 'natural', 'march', 'adventure', 'sofa', 'mind', 'piece', 'seat', 'conversing', 'your', 'hello', 'barking', 'deaf', 'race', 'meet', 're', 'fire', 'carry', 'spends', 'storms', 'follows', 'salted', 'powerful', 'butter', 'strange', 'mi', 'heals', 'april', 'stories', 'speaks', 'college', 'interests', 'flip', 'thirty', 'doesn', 'worried', 'burger', 'aids', 'sports', 'drag', 'hobby', 'rusty', 'fighter', 'clock', 'rerun', 'think', 'eleven', 'visitors', 'sent', 'poured', 'ahead', 'ordered', 'father', 'wet', 'meal', 'shaver', 'flag', 'bananas', 'sheets', 'gas', 'every', 'throw', 'teacher', 'hair', 'intend', 'mixed', 'hot', 'happened', 'eye', 'together', 'processed', 'rolled', 'toilet', 'million', 'hike', 'mcdonald', 'weren', 'turn', 'ran', 'high', 'instructions', 'wonder', 'beer', 'woman', 'neighborhood', 'harmless', 'learn', 'volume', 'sheet', 'gave', 'sucked', 'cans', 'ballot', 'chores', 'dripping', 'cows', 'fishing', 'could', 'complainers', 'los', 'pant', 'feeling', 'permit', 'cremated', 'carries', 'amazing', 'won', 'weekend', 'shorter', 'gets', 'yankees', 'saves', 'insurance', 'pink', 'gotten', 'doing', 'want', 'tan', 'eight', 'sick', 'way', 'uh', 'glass', 'nader', 'whether', 'because', 'golf', 'penny', 'cabinet', 'raised', 'steam', 'mileage', 'easter', 'tomorrow', 'cadillac', 'record', 'sink', 'follow', 'longer', 'when', 'loud', 'online', 'sneak', 'chinese', 'restaurant', 'attend', 'collision', 'bear', 'case', 'fine', 'sale', 'least', 'poems', 'maybe', 'anywhere', 'feel', 'tables', 'stuck', 'blame', 'cable', 'dive', 'friend', 'ground', 'jaywalking', 'liar', 'gone', 'except', 'next', 'home', 'seldom', 'take', 'ear', 'stands', 'andy', 'puppy', 'there', 'noses', 'glad', 'beatles', 'after', 'hotel', 'intersection', 'third', 'robbery', 'basket', 'bored', 'gun', 'dries', 'invited', 'says', 'campus', 'completely', 'asleep', 'changing', 'recent', 'cost', 'bucket', 'cell', 'float', 'jogging', 'more', 'smile', 'perfect', 'luxury', 'subject', 'santa', 'songs', 'throughout', 'island', 'lips', 'laundry', 'hamburger', 'be', 'stink', 'relax', 'cops', 'dirty', 'bait', 'baseball', 'taylors', 'solution', 'girls', 'canada', 'vitamins', 'criminals', 'small', 'hope', 'former', 'not', 'york', 'how', 'carrot', 'thinks', 'behind', 'broken', 'using', 'sign', 'grass', 'double', 'charged', 'paper', 'only', 'found', 'pcc', 'helps', 'teller', 'yearly', 'absolutely', 'buy', 'pulls', 'classes', 'give', 'fires', 'beautiful', 'thrift', 'was', 'parents', 'airport', 'please', 'weekends', 'purse', 'quiet', 'pencil', 'burst', 'cool', 'southern', 'chances', 'close', 'yes', 'newspaper', 'tall', 'carrier', 'date', 'atm', 'stone', 'caught', 'ready', 'ads', 'bedbugs', 'weird', 'hi', 'mail', 'suck', 'reservations', 'victim', 'anymore', 'tissue', 'shopping', 'leave', 'homework', 'blind', 'poor', 'plenty', 'bites', 'swiss', 'yard', 'potatoes', 'muscles', 'female', 'pineapples', 'll', 'murder', 'legs', 'stripes', 'pass', 'north', 'flipping', 'system', 'bomb', 'votes', 'horrible', 'octo', 'fat', 'stroke', 'hospitals', 'covered', 'serve', 'usually', 'protect', 'players', 'act', 'upset', 'would', 'floor', 'chips', 'musical', 'duties', 'cars', 'dollar', 'patch', 'glove', 'ripe', 'test', 'find', 'roads', 'thanks', 'heart', 'fastest', 'business', 'sharpeners', 'fault', 'bothering', 'article', 'towels', 'myself', 'fast', 'never', 'singers', 'slide', 'comes', 'diet', 'of', 'blows', 'ball', 'young', 'bottom', 'jazz', 'toast', 'lady', 'pipe', 'themselves', 'nights', 'cones', 'sticker', 'northern', 'rained', 'may', 'dinner', 'shave', 'bring', 'everyone', 'easy', 'degree', 'boiled', 'dial', 'fair', 'complain', 'killed', 'coughing', 'salt', 'rain', 'best', 'talent', 'day', 'handed', 'elephant', 'wrong', 'show', 'oranges', 'dives', 'six', 'cities', 'accidents', 'intense', 'ruin', 'inch', 'correct', 'women', 'cash', 'league', 'cracks', 'super', 'does', 'price', 'this', 'judge', 'laughed', 'pen', 'council', 'club', 'converter', 'soldier', 'organization', 'slowed', 'dictionary', 'last', 'trying', 'empty', 'holidays', 'definitely', 'brown', 'prayers', 'ended', 'expensive', 'neighbors', 'lose', 'knows', 'send', 'doctor', 'trees', 'some', 'parade', 'public', 'crack', 'street', 'firmly', 'napkin', 'elbow', 'wanted', 'legislators', 'five', 'pencils', 'exactly', 'wasn', 'bank', 'breakfast', 'smelled', 'interested', 'dog', 'government', 'die', 'cents', 'human', 'flags', 'guess', 'pillows', 'loved', 'served', 'yesterday', 'languages', 'counter', 'half', 'came', 'hoping', 'his', 'cereal', 'afraid', 'winning', 'end', 'charity', 'cap', 'lovely', 'florida', 'photos', 'climbing', 'large', 'solve', 'sweat', 'lighters', 'totally', 'wonderful', 'fans', 'smart', 'instruments', 'pushed', 'apples', 'is', 'fall', 'throwing', 'bath', 'stuffed', 'into', 'sharpener', 'payment', 'charges', 'cards', 'stomachaches', 'veterans', 'noon', 'hmm', 'ones', 'out', 'against', 'cancer', 'filled', 'shook', 'scuba', 'landed', 'tickets', 'political', 'marriage', 'files', 'tuesday', 'excellent', 'finally', 'watered', 'keys', 'wild', 'important', 'drug', 'ugly', 'bags', 'spring', 'polite', 'bluedog', 'care', 'debrah', 'roasted', 'jerk', 'constantly', 'bag', 'bought', 'lying', 'electric', 'around', 'road', 'buying', 'then', 'ralph', 'bin', 'obama', 'furthest', 'besides', 'bull', 'wife', 'cheat', 'sleepy', 'ship', 'sounds', 'hug', 'hand', 'action', 'kids', 'parking', 'should', 'step', 'alarm', 'great', 'argument', 'gained', 't', 'shoes', 'inside', 'neighborhoods', 'balloon', 'stays', 'deal', 'republicans', 'promise', 'girlfriend', 'assignments', 'june', 'california', 'loses', 'genres', 'subscription', 'handle', 'soldiers', 'congratulations', 'miss', 'calls', 'mother', 'kitchen', 'peanuts', 'wallet', 'poodles', 'glasses', 'based', 'cheer', 'cd', 'security', 'wish', 'r', 'why', 'thought', 'charge', 'an', 'honest', 'fun', 'limit', 'u', 'sensitive', 'greatest', 'meant', 'need', 'healthy', 'thieves', 'waist', 'chuck', 'hear', 'given', 'angeles', 'fit', 'month', 'snoring', 'and', 'straight', 'golfer', 'toes', 'ate', 'brush', 'las', 'already', 'officers', 'vacation', 'couple', 'desk', 'clear', 'rabbit', 'shirt', 'others', 'phony', 'tough', 'bigger', 'visit', 'switched', 'rainstorm', 'stamps', 'along', 'dome', 'unhappy', 'hard', 'finger', 'dining', 'happy', 'driving', 'birthday', 'put', 'wake', 'listen', 'story', 'wounded', 'usual', 'night', 'yuck', 'cuffs', 'game', 'neither', 'cleaning', 'little', 'offer', 'plane', 'asked', 'babe', 'outfield', 'passenger', 'difference', 'most', 'typing', 'highway', 'nothing', 'roll', 'between', 'pickles', 'ends', 'slick', 'break', 'pulled', 'language', 'stopped', 'safe', 'talk', 'friends', 'melts', 'doctors', 'hates', 'socks', 'looking', 'such', 'exciting', 'fireplaces', 'cherry', 'comfortable', 'now', 'cheater', 'silverware', 'the', 'picked', 'sleep', 'stations', 'ipod', 'joking', 'but', 'knitting', 'something', 'me', 'age', 'reporting', 'instance', 'in', 'onto', 'these', 'uncomfortable', 'button', 'official', 'oh', 'sun', 'make', 'knew', 'spend', 'manager', 'being', 'ticket', 'three', 'cloth', 'going', 'check', 'idea', 'available', 'conference', 'thinking', 'type', 'leaders', 'tournament', 'complained', 'i', 'pull', 'till', 'old', 'stomachache', 'married', 'mistake', 'leather', 'purses', 'sunday', 'off', 'salary', 'place', 'husband', 'shots', 'swine', 'shoppers', 'bill', 'peanut', 'zoo', 'just', 'excited', 'gardening', 'windows', 'kept', 'horn', 'yawning', 'paying', 'games', 'brushed', 'rest', 'promises', 'special', 'white', 'poker', 'warhol', 'ham', 'paid', 'repairman', 'flu', 'river', 'selling', 'bit', 'often', 'tag', 'mostly', 'different', 'monument', 'simple', 'grow', 'yikes', 'fighting', 's', 'coat', 'puts', 'holding', 'meat', 'banana', 'change', 'playing', 'ocean', 'taxi', 'attending', 'unpredictable', 'believes', 'rid', 'macs', 'elected', 'until', 'left', 'somebody', 'jokes', 'dvd', 'chilly', 'rowed', 'crime', 'jerks', 'they', 'newspapers', 'peeled', 'family', 'cats', 'wiped', 'variety', 'waste', 'call', 'those', 'state', 'crashed', 'internet', 'odors', 'truck', 'officer', 'trail', 'arizona', 'order', 'crowds', 'costs', 'flew', 'chairs', 'paint', 'hassle', 'actress', 'height', 'department', 'especially', 'belongings', 'robber', 'too', 'anyway', 'who', 'january', 'power', 'tie', 'bacon', 'restaurants', 'beard', 'pilot', 'answer', 'ten', 'baby', 'trash', 'got', 'curse', 'awesome', 'means', 'drivers', 'true', 'forever', 'spare', 'loads', 'stunk', 'been', 'house', 'leaves', 'sugar', 'receipt', 'replace', 'plate', 'shoot', 'digital', 'soon', 'mule', 'lizards', 'ounces', 'puddle', 'grade', 'changes', 'hole', 'stinks', 'big', 'entire', 'blood', 'park', 'slowing', 'pens', 'officials', 'looks', 'year', 'taught', 'identify', 'far', 'yell', 'miles', 'living', 'measures', 'were', 'person', 'private', 'jet', 'nonfat', 'interesting', 'steal', 'told', 'lock', 'flying', 'delicious', 'washed', 'tree', 'during', 'clerk', 'cooking', 'depends', 'screws', 'painting', 'weight', 'yelling', 'him', 'holes', 'point', 'differently', 'laugh', 'well', 'hit', 'rinse', 'pet', 'personal', 'original', 'theaters', 'ill', 'violence', 'come', 'invite', 'hey', 'usc', 'freezer', 'mom', 'fill', 'money', 'pretty', 'imagine', 'mirror', 'haven', 'corner', 'dryer', 'workers', 'knocking', 'funny', 'run', 'knife', 'war', 'hasn', 'cheap', 'love', 'promised', 'missing', 'judy', 'visiting', 'germs', 'skin', 'american', 'aisle', 'on', 'burned', 'drive', 'better', 'anniversary', 'swam', 'sneezing', 'our', 'open', 'pound', 'fix', 'doorbell', 'rather', 'cups', 'pour', 'drinking', 'cute', 'layoffs', 'writing', 'hamburgers', 'spent', 'hire', 'police', 'smells', 'cabin', 'envelope', 'someone', 'four', 'earlier', 'strong', 'commercials', 'pieces', 'highways', 'personality', 'waiting', 'finding', 'terrible', 'cent', 'player', 'reelection', 'took', 'speed', 'mouth', 'yards', 'screen', 'didn', 'very', 'supermarket', 'really', 'tried', 'car', 'd', 'song', 'hawaii', 'able', 'windy', 'forget', 'grand', 'free', 'color', 'drown', 'hobbies', 'than', 'planned', 'careful', 'started', 'lemon', 'blinking', 'twelve', 'tub', 'shined', 'kind', 'perfectly', 'slowest', 'picking', 'routine', 'problems', 'agree', 'streets', 'dictionaries', 'donate', 'traveling', 'second', 'travel', 'puff', 'tight', 'might', 'tells', 'use', 'shut', 'events', 'few', 'appreciate', 'near', 'brings', 'sky', 'blades', 'jets', 'lie', 'books', 'punched', 'without', 'medicine', 'election', 'understand', 'tvs', 'quit', 'pointless', 'likes', 'company', 'like', 'first', 'while', 'attended', 'warm', 'art', 'loosen', 'deep', 'round', 'group', 'stretch', 'air', 'any', 'climb', 'liked', 'minutes', 'sense', 'team', 'salad', 'president', 'lane', 'my', 'inspection', 'read', 'its', 'drives', 'driver', 'lost', 'awake', 'born', 'evening', 'actors', 'card', 'pimples', 'he', 'list', 'save', 'drops', 'gives', 'friendly', 'cream', 'enjoying', 'recorder', 'smoke', 'excuse', 'win', 'picasso', 'known', 'allergic', 'bumped', 'help', 'lesson', 'food', 'beds', 've', 'suntan', 'lifeguard', 'believe', 'buckle', 'pepper', 'again', 'hiking', 'taxes', 'apologize', 'canceled', 'putt', 'yells', 'types', 'disappear', 'life', 'sidewalk', 'speak', 'letter', 'odd', 'own', 'shakespeare', 'wall', 'saw', 'short', 'if', 'seems', 'hospital', 'library', 'coffee', 'cooks', 'saturday', 'smelling', 'ideas', 'bismol', 'forty', 'try', 'appointment', 'building', 'missed', 'service', 'share', 'supervisor', 'elevator', 'pasta', 'pasadena', 'arrow', 'once', 'truth', 'clears', 'elastic', 'incomplete', 'shirts', 'here', 'daddy', 'per', 'set', 'space', 'don', 'offering', 'return', 'pick', 'sealed', 'kill', 'wipe', 'sample', 'sooner', 'plan', 'bulb', 'teach', 'broke', 'move', 'bark', 'graduate', 'see', 'example', 'months', 'swimming', 'aches', 'temperature', 'died', 'signal', 'pepperoni', 'actually', 'get', 'seatbelt', 'top', 'schools', 'crud', 'unable', 'mac', 'right', 'jobs', 'much', 'look', 'late', 'mailing', 'cover', 'lessons', 'seven', 'eats', 'shaken', 'mouse', 'land', 'remove', 'bread', 'safer', 'busy', 'prisons', 'homeless', 'king', 'also', 'conditioner', 'title', 'sight', 'invading', 'pale', 'pimple', 'borrow', 'hang', 'stay', 'measure', 'reason', 'brand', 'bite', 'anyone', 'stains', 'wouldn', 'dead', 'stronger', 'grandma', 'bother', 'nosey', 'distance', 'movies', 'sucks', 'walls', 'questions', 'invites', 'worse', 'doors', 'code', 'fridge', 'sorry', 'drop', 'spot', 'bar', 'puppies', 'downtown', 'watched', 'nope', 'force', 'summer', 'ink', 'sank', 'dodgers', 'cuts', 'chase', 'tens', 'sausage', 'raising', 'firebug', 'ruth', 'weak', 'hours', 'we', 'pants', 'it', 'town', 'occur', 'breaks', 'do', 'anything', 'famous', 'laptop', 'shelter', 'making', 'site', 'warned', 'played', 'which', 'jessica', 'fourth', 'showed', 'yelled', 'stupid', 'water', 'keep', 'hotter', 'seats', 'yellow', 'ask', 'features', 'rains', 'iron', 'pc', 'morning', 'allowed', 'fish', 'though', 'mm', 'trade', 'bedrooms', 'allow', 'nicer', 'mention', 'hour', 'current', 'reliable', 'clean', 'senator', 'cop', 'hurry', 'possibly', 'destroyed', 'taste', 'considering', 'p', 'rooms', 'responsibility', 'switching', 'eating', 'match', 'jail', 'drawing', 'black', 'mood', 'ripped', 'poet', 'stress', 'coming', 'passed', 'sandwiches', 'has', 'hurricanes', 'remote', 'red', 'huge', 'interview', 'start', 'did', 'juice', 'eggs', 'calculators', 'shaving', 'accident', 'mental', 'seller', 'woods', 'disgusting', 'metal', 'tonight', 'yourself', 'boy', 'alone', 'slip', 'units', 'job', 'answered', 'hurt', 'orange', 'property', 'bathrooms', 'real', 'brother', 'zip', 'battery', 'kidding', 'noise', 'percent', 'wasting', 'trouble', 'macy', 'faucet', 'certainly', 'quickly', 'democrats', 'cousin', 'exercise', 'serious', 'starts', 'yy', 'funniest', 'cheese', 'lake', 'shot', 'nose', 'random', 'prepare', 'warmer', 'afternoon', 'superbad', 'door', 'years', 'hood', 'them', 'realize', 'country', 'lend', 'berries', 'from', 'bad', 'felt', 'figure', 'regardless', 'ninety', 'her', 'worth', 'wheelchairs', 'footer', 'occasionally', 'impossible', 'emergency', 'sit', 'shouldn', 'smokes', 'welcome', 'hate', 'report', 'avoid', 'tip', 'pursuit', 'cat', 'bears', 'smokers', 'twice', 'buses', 'mayor', 'week', 'reading', 'walked', 'begin', 'probably', 'markets', 'pillowcases', 'cleaner', 'broadcasting', 'are', 'pcs', 'lucky', 'opposite', 'okay', 'mad', 'ii', 'trucks', 'feed', 'causes', 'guessing', 'instantly', 'catches', 'oil', 'talented', 'become', 'radios', 'gray', 'fruit', 'bumps', 'pollution', 'worry', 'problem', 'whole', 'telling', 'switch', 'inviting', 'alice', 'registration', 'average', 'activities', 'children', 'coworkers', 'across', 'describe', 'birds', 'final', 'followed', 'bases', 'bleeding', 'chucks'}\n",
            "{'forecast', 'm', 'cut', 'figures', 'channels', 'flight', 'ashes', 'speech', 'cartoons', 'address', 'yet', 'for', 'speeding', 'unbelievable', 'jar', 'am', 'medication', 'license', 'stole', 'print', 'rude', 'golfers', 'folded', 'rules', 'deserved', 'housekeeping', 'chop', 'explain', 'patients', 'tears', 'couldn', 'floors', 'both', 'long', 'sitting', 'colder', 'fingers', 'extra', 'various', 'things', 'noticed', 'snow', 'two', 'unit', 'insert', 'crying', 'falling', 'christmas', 'stopping', 'suction', 'adding', 'rich', 'school', 'quite', 'god', 'navel', 'plus', 'news', 'wipes', 'reuse', 'stops', 'jacket', 'handyman', 'court', 'savings', 'hitting', 'students', 'survive', 'nation', 'artist', 'feelings', 'squeeze', 'sliced', 'focus', 'guarantees', 'rose', 'swear', 'at', 'rush', 'prices', 'butterflies', 'smoker', 'nuts', 'learned', 'crowded', 'will', 'uses', 'added', 'cheating', 'standing', 'somewhere', 'potato', 'vacuumed', 'siren', 'degrees', 'poodle', 'overseas', 'seeing', 'under', 'crosswalk', 'worst', 'shows', 'killer', 'cart', 'licks', 'yeah', 'parties', 'visited', 'star', 'hopefully', 'earth', 'aid', 'always', 'ruined', 'son', 'wearing', 'sandwich', 'automatic', 'mall', 'stand', 'animals', 'reservation', 'twenty', 'hitter', 'question', 'blade', 'gently', 'takes', 'loves', 'drew', 'crazy', 'that', 'hands', 'pepto', 'damp', 'smell', 'checkbook', 'guys', 'witness', 'window', 'voted', 'ever', 'pets', 'starbucks', 'tasted', 'laughing', 'watch', 'gravity', 'cross', 'went', 'sprinkled', 'green', 'favor', 'ebay', 'drink', 'same', 'wood', 'add', 'artists', 'away', 'cook', 'parked', 'nice', 'band', 'brought', 'vacuum', 'outer', 'celery', 'lot', 'snowman', 'lawns', 'voters', 'dry', 'clearly', 'subscribed', 'through', 'apartment', 'waistband', 'boil', 'tape', 'live', 'class', 'coffin', 'genes', 'many', 'either', 'another', 'cigarette', 'reporters', 'shoe', 'plans', 'trust', 'fell', 'flower', 'calling', 'outside', 'honor', 'sand', 'slow', 'alley', 'hungry', 'thin', 'stood', 'sew', 'hell', 'about', 'asking', 'jammed', 'market', 'lasts', 'rosters', 'rub', 'flat', 'prefer', 'washings', 'a', 'sleeves', 'good', 'since', 'minute', 'catch', 'brushing', 'angry', 'understands', 'rice', 'desktop', 'stomach', 'dd', 'matter', 'store', 'used', 'before', 'bless', 'bone', 'nails', 'houses', 'starving', 'sharp', 'seriously', 'teaching', 'eat', 'soup', 'wins', 'listening', 'mars', 'towel', 'tutor', 'head', 'ha', 'invitation', 'buyer', 'knee', 'cares', 'china', 'had', 'carton', 'wear', 'damage', 'where', 'weather', 'thousand', 'pounds', 'stamp', 'name', 'having', 'watchdogs', 'soap', 'early', 'joke', 'anytime', 'bush', 'silly', 'bowling', 'everything', 'mean', 'heaven', 'planes', 'know', 'she', 'invitations', 'pocket', 'cold', 'other', 'world', 'working', 'down', 'hysterically', 'their', 'rob', 'reminder', 'facial', 'nonsmoking', 'army', 'stop', 'time', 'knock', 'phone', 'hilarious', 'tires', 'marks', 'immediately', 'loosened', 'google', 'face', 'bed', 'goes', 'planning', 'wants', 'today', 'basketball', 'controls', 'uncertain', 'dying', 'raw', 'enough', 'almost', 'lots', 'waiter', 'unfriendly', 'practice', 'helped', 'heat', 'machine', 'hurts', 'tax', 'party', 'made', 'by', 'birth', 'music', 'ears', 'man', 'room', 'glue', 'magazines', 'pages', 'everybody', 'pair', 'increase', 'sharpen', 'mine', 'french', 'times', 'computers', 'remind', 'herself', 'main', 'pizza', 'pictures', 'checked', 'lately', 'cruise', 'positive', 'cheaper', 'cause', 'entered', 'over', 'none', 'arm', 'ring', 'lunch', 'closer', 'blow', 'sunny', 'belong', 'attitude', 'unless', 'rate', 'plants', 'honestly', 'continued', 'unlucky', 'popular', 'isn', 'customer', 'textbooks', 'rhyme', 'taken', 'book', 'monthly', 'fallen', 'canyon', 'supposed', 'cry', 'sell', 'learning', 'victorious', 'deserve', 'credit', 'pockets', 'all', 'actor', 'anita', 'common', 'exit', 'goats', 'plastic', 'voting', 'slips', 'sticks', 'goodness', 'turning', 'seem', 'sure', 'days', 'film', 'drinks', 'unsafe', 'drove', 'flown', 'can', 'certain', 'babies', 'gambling', 'iq', 'belt', 'people', 'ago', 'mattress', 'classroom', 'sort', 'surgery', 'dozen', 'later', 'course', 'bitter', 'withdraw', 'line', 'spanish', 'examine', 'ridiculous', 'afterwards', 'carts', 'sad', 'new', 'must', 'scrub', 'figured', 'wind', 'whatever', 'shop', 'sleeve', 'initial', 'breath', 'number', 'knuckles', 'turned', 'go', 'mustard', 'deals', 'beach', 'teachers', 'fruits', 'everywhere', 'yours', 'section', 'unbuttoned', 'english', 'low', 'reheated', 'sometimes', 'promotion', 'carpet', 'bathroom', 'spit', 'vegetables', 'dad', 'earplugs', 'basically', 'vegas', 'freeways', 'walk', 'buys', 'eyes', 'whenever', 'hold', 'cone', 'showers', 'painter', 'weighs', 'mexico', 'boyfriend', 'cemetery', 'normal', 'salads', 'funeral', 'debit', 'guy', 'channel', 'winter', 'favorite', 'happens', 'lettuce', 'word', 'babysitter', 'prescription', 'tell', 'strict', 'owned', 'invented', 'changed', 'dream', 'weatherman', 'milk', 'earthquakes', 'apologized', 'nearby', 'stepladder', 'crash', 'each', 'tasty', 'steak', 'drugs', 'us', 'what', 'seen', 'chain', 'blue', 'draw', 'faster', 'stove', 'heard', 'score', 'hp', 'swelling', 'closed', 'buried', 'dogs', 'tests', 'looked', 'wash', 'steps', 'harsh', 'up', 'bus', 'friday', 'andrew', 'sponge', 'honk', 'so', 'running', 'cow', 'ca', 'improve', 'no', 'corporations', 'or', 'boss', 'candy', 'joining', 'breathe', 'stock', 'threat', 'customs', 'washes', 'retires', 'sewing', 'reviews', 'drowned', 'ice', 'lonely', 'finished', 'hundred', 'teeth', 'dressing', 'thirteenth', 'heated', 'guns', 'lawsuits', 'kid', 'carrying', 'you', 'chased', 'lead', 'happyness', 'points', 'shelf', 'causing', 'season', 'else', 'luck', 'taking', 'foot', 'enjoyed', 'email', 'remember', 'racist', 'as', 'fingernails', 'passes', 'explode', 'hidden', 'watching', 'raining', 'dating', 'backpack', 'sides', 'democratic', 'afford', 'recently', 'moved', 'saying', 'sister', 'table', 'sometime', 'tissues', 'homes', 'chemistry', 'amateur', 'major', 'nervous', 'floss', 'speaking', 'packs', 'math', 'scratching', 'law', 'movie', 'dangerous', 'concrete', 'men', 'pbs', 'fortune', 'speaker', 'checkout', 'mountains', 'alive', 'done', 'gloves', 'play', 'shells', 'stayed', 'honda', 'poetry', 'divorced', 'chance', 'sound', 'shine', 'needs', 'calculator', 'girl', 'daughter', 'front', 'bet', 'trip', 'candidate', 'forgot', 'chocolate', 'perfume', 'produce', 'wow', 'mccain', 'tiger', 'telephones', 'instead', 'voter', 'classmates', 'possible', 'forgetting', 'mud', 'death', 'sudden', 'aren', 'mechanic', 'dollars', 'parts', 'feet', 'oops', 'arms', 'nobody', 'rubber', 'dent', 'outfit', 'dentist', 'pray', 'boring', 'o', 'fact', 'light', 'called', 'vote', 'poke', 'middle', 'wait', 'numbers', 'one', 'full', 'tomato', 'foul', 'sidewalks', 'still', 'saving', 'relaxed', 'labor', 'thing', 'planet', 'easier', 'suspended', 'student', 'single', 'nap', 'work', 'toss', 'woke', 'tired', 'tv', 'have', 'shake', 'dark', 'burnt', 'raise', 'makes', 'prettiest', 'drawer', 'rods', 'traffic', 'forgive', 'to', 'killing', 'computer', 'rock', 'troublemakers', 'payments', 'laid', 'soft', 'b', 'offered', 'respect', 'riding', 'flossing', 'radio', 'fresh', 'say', 'antenna', 'met', 'back', 'complaining', 'said', 'finish', 'boobs', 'cigarettes', 'lungs', 'burner', 'upstairs', 'october', 'prove', 'smoking', 'city', 'badly', 'recycle', 'altitude', 'thank', 'office', 'quietly', 'meowing', 'works', 'with', 'values', 'stuff', 'write', 'chore', 'crossing', 'sleeping', 'part', 'frequently', 'church', 'saved', 'notebook', 'getting', 'leftovers', 'installation', 'let', 'pain', 'view', 'placed', 'paintings', 'pay', 'painted', 'pleasant', 'decided', 'diapers', 'unplug', 'stars', 'dried', 'injured', 'enjoy', 'talking', 'fireman', 'latest', 'even', 'f', 'theater', 'skills', 'hunch', 'happen', 'spoke', 'sing', 'kinds', 'doubt', 'natural', 'march', 'adventure', 'sofa', 'mind', 'piece', 'seat', 'your', 'hello', 'barking', 'deaf', 'race', 'meet', 're', 'fire', 'sat', 'storms', 'carry', 'spends', 'follows', 'salted', 'butter', 'strange', 'mi', 'heals', 'april', 'stories', 'speaks', 'college', 'interests', 'flip', 'thirty', 'doesn', 'worried', 'burger', 'aids', 'drag', 'sports', 'hobby', 'rusty', 'fighter', 'clock', 'rerun', 'sore', 'think', 'eleven', 'visitors', 'sent', 'poured', 'ahead', 'ordered', 'father', 'wet', 'meal', 'shaver', 'discount', 'flag', 'bananas', 'sheets', 'gas', 'every', 'teacher', 'intend', 'hair', 'throw', 'mixed', 'hot', 'happened', 'eye', 'processed', 'together', 'rolled', 'toilet', 'million', 'hike', 'mcdonald', 'turn', 'ran', 'high', 'instructions', 'wonder', 'woman', 'beer', 'neighborhood', 'learn', 'harmless', 'volume', 'sheet', 'gave', 'sucked', 'cans', 'ballot', 'chores', 'dripping', 'cows', 'fishing', 'could', 'sniff', 'complainers', 'los', 'pant', 'feeling', 'permit', 'cremated', 'carries', 'amazing', 'won', 'weekend', 'yankees', 'gets', 'shorter', 'saves', 'insurance', 'pink', 'gotten', 'doing', 'want', 'tan', 'eight', 'sick', 'way', 'uh', 'bars', 'glass', 'nader', 'whether', 'because', 'golf', 'penny', 'raised', 'cabinet', 'soak', 'steam', 'mileage', 'easter', 'tomorrow', 'cadillac', 'record', 'sink', 'follow', 'longer', 'when', 'loud', 'online', 'sneak', 'chinese', 'restaurant', 'collision', 'attend', 'bear', 'case', 'fine', 'sale', 'least', 'poems', 'maybe', 'anywhere', 'feel', 'tables', 'stuck', 'blame', 'cable', 'dive', 'friend', 'ground', 'jaywalking', 'liar', 'gone', 'except', 'next', 'home', 'seldom', 'take', 'ear', 'stands', 'andy', 'puppy', 'there', 'glad', 'after', 'third', 'intersection', 'hotel', 'robbery', 'basket', 'gun', 'dries', 'invited', 'completely', 'says', 'campus', 'asleep', 'changing', 'recent', 'cost', 'bucket', 'cell', 'float', 'more', 'smile', 'perfect', 'luxury', 'subject', 'santa', 'happiness', 'songs', 'throughout', 'island', 'himself', 'lips', 'laundry', 'hamburger', 'be', 'stink', 'shooting', 'relax', 'cops', 'dirty', 'bait', 'baseball', 'taylors', 'solution', 'clothes', 'caused', 'girls', 'canada', 'vitamins', 'criminals', 'small', 'hope', 'not', 'york', 'how', 'carrot', 'thinks', 'behind', 'broken', 'using', 'grass', 'sign', 'double', 'charged', 'paper', 'only', 'found', 'pcc', 'helps', 'teller', 'yearly', 'absolutely', 'buy', 'fight', 'pulls', 'classes', 'give', 'fires', 'beautiful', 'thrift', 'was', 'parents', 'airport', 'please', 'purse', 'weekends', 'quiet', 'pencil', 'burst', 'cool', 'southern', 'chances', 'close', 'yes', 'newspaper', 'tall', 'date', 'atm', 'stone', 'caught', 'ready', 'ads', 'bedbugs', 'weird', 'mail', 'suck', 'victim', 'reservations', 'anymore', 'tissue', 'shopping', 'leave', 'homework', 'blind', 'poor', 'plenty', 'millions', 'bites', 'swiss', 'yard', 'potatoes', 'muscles', 'less', 'female', 'pineapples', 'll', 'murder', 'legs', 'stripes', 'pass', 'north', 'losing', 'flipping', 'system', 'votes', 'bomb', 'horrible', 'octo', 'fat', 'stroke', 'hospitals', 'covered', 'serve', 'usually', 'protect', 'players', 'act', 'upset', 'would', 'floor', 'chips', 'musical', 'duties', 'cars', 'dollar', 'patch', 'glove', 'ripe', 'test', 'find', 'roads', 'thanks', 'heart', 'fastest', 'business', 'sharpeners', 'fault', 'bothering', 'article', 'towels', 'myself', 'fast', 'never', 'singers', 'slide', 'diet', 'comes', 'of', 'blows', 'letting', 'ball', 'young', 'bottom', 'jazz', 'toast', 'lady', 'pipe', 'themselves', 'nights', 'cones', 'sticker', 'northern', 'rained', 'side', 'may', 'dinner', 'huh', 'shave', 'bring', 'everyone', 'easy', 'degree', 'boiled', 'crashes', 'dial', 'fair', 'complain', 'killed', 'coughing', 'salt', 'rain', 'best', 'talent', 'day', 'handed', 'elephant', 'wrong', 'show', 'oranges', 'dives', 'six', 'cities', 'accidents', 'intense', 'ruin', 'cash', 'correct', 'women', 'inch', 'league', 'cracks', 'super', 'does', 'price', 'this', 'judge', 'pennies', 'laughed', 'pen', 'ahora', 'council', 'club', 'converter', 'organization', 'soldier', 'slowed', 'dictionary', 'last', 'trying', 'empty', 'holidays', 'definitely', 'brown', 'prayers', 'ended', 'expensive', 'neighbors', 'lose', 'knows', 'trees', 'doctor', 'send', 'some', 'parade', 'public', 'crack', 'street', 'firmly', 'napkin', 'legislators', 'wanted', 'elbow', 'five', 'pencils', 'wasn', 'exactly', 'bank', '<START>', 'breakfast', 'smelled', 'interested', 'dog', 'government', 'die', 'cents', 'human', 'flags', 'pillows', 'guess', 'loved', 'served', 'yesterday', 'languages', 'counter', 'half', 'came', 'hoping', 'his', 'cereal', 'afraid', 'winning', 'end', 'charity', 'cap', 'lovely', 'florida', 'photos', 'climbing', 'large', 'solve', 'sweat', 'lighters', 'totally', 'wonderful', 'fans', 'smart', 'instruments', 'pushed', 'apples', 'is', 'fall', 'throwing', 'bath', 'into', 'sharpener', 'payment', 'charges', 'cards', 'stomachaches', 'pop', 'veterans', 'noon', 'hmm', 'ones', 'out', 'against', 'america', 'cancer', 'filled', 'drunks', 'shook', 'scuba', 'landed', 'tickets', 'political', 'marriage', 'files', 'tuesday', 'excellent', 'finally', 'watered', 'wild', 'keys', 'important', 'drug', 'ugly', 'bags', 'bluedog', 'polite', 'care', 'debrah', 'roasted', 'bag', 'constantly', 'jerk', 'bought', 'electric', 'lying', 'around', 'road', 'then', 'ralph', 'obama', 'furthest', 'besides', 'bull', 'wife', 'cheat', 'sleepy', 'sounds', 'predictable', 'heal', 'hand', 'action', 'kids', 'parking', 'should', 'step', 'alarm', 'hurricane', 'great', 'argument', 'gained', 't', 'shoes', 'inside', 'balloon', 'stays', 'deal', 'promise', 'republicans', 'girlfriend', 'assignments', 'june', 'california', 'loses', 'genres', 'subscription', 'handle', 'soldiers', 'congratulations', 'miss', 'calls', 'mother', 'kitchen', 'peanuts', 'wallet', 'poodles', 'glasses', 'based', 'cheer', 'cd', 'agent', 'security', 'wish', 'r', 'why', 'thought', 'charge', 'an', 'honest', 'fun', 'limit', 'u', 'sensitive', 'greatest', 'meant', 'need', 'healthy', 'thieves', 'waist', 'chuck', 'hear', 'given', 'angeles', 'fit', 'month', 'cds', 'snoring', 'and', 'straight', 'golfer', 'toes', 'ate', 'brush', 'las', 'already', 'officers', 'couple', 'desk', 'clear', 'rabbit', 'shirt', 'others', 'phony', 'tough', 'bigger', 'visit', 'switched', 'rainstorm', 'stamps', 'along', 'dome', 'unhappy', 'hard', 'finger', 'dining', 'happy', 'driving', 'birthday', 'put', 'wake', 'listen', 'story', 'wounded', 'e', 'usual', 'cuffs', 'night', 'yuck', 'game', 'neither', 'cleaning', 'little', 'offer', 'plane', 'asked', 'babe', 'outfield', 'passenger', 'difference', 'most', 'typing', 'highway', 'nothing', 'roll', 'between', 'pickles', 'ends', 'pulled', 'break', 'slick', 'quicker', 'language', 'stopped', 'safe', 'politicians', 'friends', 'talk', 'melts', 'doctors', 'hates', 'socks', 'looking', 'such', 'exciting', 'fireplaces', 'cherry', 'comfortable', 'now', 'cheater', 'silverware', 'the', 'picked', 'sleep', 'ipod', 'joking', 'but', 'knitting', 'something', 'me', 'age', 'instance', 'reporting', 'in', 'onto', 'these', 'uncomfortable', 'button', 'oh', 'sun', 'make', 'knew', 'spend', 'manager', 'being', 'ticket', 'blacky', 'three', 'cloth', 'going', 'idea', 'check', 'thinking', 'type', 'leaders', 'tournament', 'complained', 'i', 'pull', 'till', 'towed', 'old', 'stomachache', 'married', 'mistake', 'leather', 'purses', 'sunday', 'off', 'salary', 'place', 'husband', 'shots', 'swine', 'shoppers', 'bill', 'peanut', 'zoo', 'just', 'gardening', 'excited', 'windows', 'kept', 'horn', 'yawning', 'paying', 'games', 'brushed', 'rest', 'promises', 'special', 'white', 'poker', 'warhol', 'ham', 'paid', 'repairman', 'flu', 'river', 'selling', 'bit', 'often', 'tag', 'mostly', 'different', 'monument', 'simple', 'grow', 'yikes', 'fighting', 's', 'coat', 'puts', 'holding', 'meat', 'banana', 'change', 'playing', 'ocean', 'taxi', 'attending', 'unpredictable', 'snails', 'believes', 'rid', 'macs', 'elected', 'until', 'left', 'somebody', 'jokes', 'dvd', 'chilly', 'crime', 'jerks', 'they', 'peeled', 'newspapers', 'family', 'cats', 'wiped', 'waste', 'variety', 'call', 'those', 'state', 'crashed', 'internet', 'odors', 'truck', 'officer', 'trail', 'arizona', 'order', 'crowds', 'chairs', 'flew', 'costs', 'paint', 'male', 'hassle', 'actress', 'height', 'department', 'especially', 'belongings', 'robber', 'too', 'anyway', 'who', 'january', 'power', 'tie', 'bacon', 'restaurants', 'beard', 'pilot', 'answer', 'ten', 'baby', 'trash', 'got', 'curse', 'means', 'awesome', 'drivers', 'true', 'forever', 'loads', 'stunk', 'been', 'house', 'leaves', 'sugar', 'receipt', 'replace', 'plate', 'shoot', 'digital', 'soon', 'mule', 'lizards', 'ounces', 'puddle', 'grade', 'changes', 'hole', 'stinks', 'big', 'entire', 'blood', 'park', 'slowing', 'officials', 'pens', 'looks', 'year', 'taught', 'identify', 'far', 'yell', 'miles', 'living', 'person', 'were', 'measures', 'private', 'jet', 'nonfat', 'interesting', 'steal', 'goodbye', 'told', 'lock', 'flying', 'delicious', 'washed', 'tree', 'during', 'clerk', 'cooking', 'bright', 'depends', 'dies', 'screws', 'painting', 'weight', 'yelling', 'him', 'holes', 'differently', 'point', 'laugh', 'apply', 'well', 'hit', 'rinse', 'pet', 'personal', 'original', 'theaters', 'ill', 'violence', 'come', 'invite', 'yahoo', 'honey', 'usc', 'freezer', 'mom', 'fill', 'money', 'pretty', 'imagine', 'mirror', 'giving', 'haven', 'corner', 'dryer', 'workers', 'knocking', 'funny', 'run', 'knife', 'shower', 'war', 'hasn', 'cheap', 'love', 'promised', 'missing', 'judy', 'visiting', 'germs', 'skin', 'american', 'aisle', 'on', 'burned', 'drive', 'better', 'swam', 'anniversary', 'reelected', 'sneezing', 'our', 'pound', 'open', 'fix', 'doorbell', 'rather', 'cups', 'pour', 'drinking', 'cute', 'layoffs', 'writing', 'hamburgers', 'spent', 'hire', 'police', 'smells', 'cabin', 'envelope', 'someone', 'four', 'nanny', 'earlier', 'strong', 'commercials', 'pieces', 'highways', 'personality', 'waiting', 'finding', 'terrible', 'camera', 'cent', 'reelection', 'player', 'took', 'speed', 'mouth', 'yards', 'screen', 'didn', 'very', 'supermarket', 'really', 'tried', 'car', 'd', 'umbrella', 'bye', 'song', 'hawaii', 'able', 'free', 'forget', 'grand', 'color', 'drown', 'than', 'planned', 'careful', 'started', 'lemon', 'blinking', 'twelve', 'tub', 'shined', 'kind', 'perfectly', 'slowest', 'picking', 'search', 'routine', 'sexist', 'problems', 'agree', 'streets', 'second', 'dictionaries', 'traveling', 'donate', 'travel', 'puff', 'tight', 'might', 'tells', 'use', 'shut', 'events', 'few', 'appreciate', 'brings', 'sky', 'blades', 'lie', 'jets', 'books', 'punched', 'without', 'modern', 'medicine', 'election', 'understand', 'tvs', 'quit', 'pointless', 'likes', 'infants', 'company', 'like', 'first', 'while', 'attended', 'warm', 'art', 'loosen', 'deep', 'round', 'group', 'air', 'any', 'climb', 'liked', 'minutes', 'sense', 'team', 'leads', 'salad', 'president', 'lane', 'my', 'inspection', 'read', 'its', 'drives', 'driver', 'lost', 'awake', 'born', 'actors', 'evening', 'card', 'pimples', 'he', 'list', 'save', 'drops', 'gives', 'friendly', 'cream', 'enjoying', 'recorder', 'smoke', 'excuse', 'win', 'picasso', 'known', 'allergic', 'bumped', 'help', 'lesson', 'food', 'beds', 've', 'lifeguard', 'believe', 'pepper', 'buckle', 'again', 'hiking', 'taxes', 'apologize', 'canceled', 'putt', 'yells', 'types', 'disappear', 'korean', 'life', 'sidewalk', 'speak', 'letter', 'odd', 'own', 'shakespeare', 'wall', 'saw', 'tastes', 'short', 'if', 'seems', 'hospital', 'library', 'coffee', 'cooks', 'saturday', 'smelling', 'ideas', 'bismol', 'forty', 'try', 'appointment', 'building', 'missed', 'service', 'share', 'supervisor', 'elevator', 'pasta', 'pasadena', 'arrow', 'once', 'pickle', 'truth', 'clears', 'elastic', 'incomplete', 'shirts', 'here', 'daddy', 'space', 'set', 'per', 'don', 'offering', 'return', 'pick', 'sealed', 'kill', 'wipe', 'lick', 'sooner', 'plan', 'bulb', 'arguing', 'teach', 'broke', 'move', 'bark', 'graduate', 'see', 'example', 'months', 'swimming', 'aches', 'temperature', 'died', 'signal', 'pepperoni', 'actually', 'get', 'seatbelt', 'top', 'schools', 'crud', 'unable', 'mac', 'right', 'jobs', 'much', 'look', 'late', 'mailing', 'cover', 'lessons', 'seven', 'eats', 'shaken', 'mouse', 'land', 'remove', 'bread', 'safer', 'busy', 'prisons', 'homeless', 'king', 'also', 'conditioner', 'title', 'sight', 'pale', 'pimple', 'borrow', 'hang', 'stay', 'measure', 'reason', 'brand', 'bite', 'anyone', 'stains', 'wouldn', 'dead', 'stronger', 'repeat', 'bother', 'nosey', 'grandma', 'distance', 'movies', 'walls', 'invites', 'questions', 'sucks', 'worse', 'doors', 'code', 'fridge', 'sorry', 'drop', 'ending', 'spot', 'bar', 'puppies', 'downtown', 'watched', 'nope', 'force', 'mommy', 'summer', 'ink', 'sank', 'patient', 'dodgers', 'cuts', 'chase', 'tens', 'sausage', 'moves', 'raising', 'firebug', 'ruth', 'weak', 'hours', 'we', 'donations', 'bills', 'pants', 'it', 'town', 'occur', 'breaks', 'do', 'anything', 'famous', 'laptop', 'making', 'site', 'warned', 'played', 'which', 'fourth', 'showed', 'yelled', 'stupid', 'water', 'keep', 'hotter', 'seats', 'yellow', 'ask', 'rains', 'iron', 'features', 'pc', 'morning', 'allowed', 'fish', 'though', 'trade', 'mm', 'choose', 'bedrooms', 'allow', 'mention', 'hour', 'current', 'reliable', 'clean', 'cop', 'hurry', 'possibly', 'destroyed', 'taste', 'considering', 'p', 'rooms', 'responsibility', 'switching', 'eating', 'match', 'jail', 'drawing', 'black', 'mood', 'poet', 'stress', 'coming', 'passed', 'sandwiches', 'has', 'hurricanes', 'remote', 'red', 'huge', 'someday', 'start', 'did', 'juice', 'eggs', 'calculators', 'shaving', 'accident', 'mental', 'zzz', 'seller', 'woods', 'metal', 'tonight', 'disgusting', 'yourself', 'boy', 'alone', 'slip', 'units', 'job', 'answered', 'hurt', 'orange', 'property', 'bathrooms', 'real', 'bubbles', 'brother', 'zip', 'battery', 'kidding', 'noise', 'percent', 'wasting', 'trouble', 'macy', 'faucet', 'quickly', 'certainly', 'democrats', 'cousin', 'exercise', 'serious', 'starts', 'yy', 'funniest', 'cheese', 'com', 'lake', 'shot', 'nose', 'random', 'prepare', 'warmer', 'afternoon', 'superbad', 'door', 'years', 'hood', 'them', 'realize', 'country', 'lend', 'berries', 'bases', 'bad', 'from', 'felt', 'figure', 'regardless', 'ninety', 'mop', 'her', 'worth', 'wheelchairs', 'footer', 'occasionally', 'emergency', 'impossible', 'sit', 'shouldn', 'hate', 'welcome', 'report', 'avoid', 'tip', 'pursuit', 'cat', 'bears', 'smokers', 'twice', 'mayor', 'buses', 'week', 'reading', 'walked', 'begin', 'probably', 'markets', 'pillowcases', 'cleaner', 'broadcasting', 'are', 'pcs', 'lucky', 'opposite', 'okay', 'mad', 'ii', 'trucks', 'feed', 'causes', 'guessing', '<END>', 'instantly', 'catches', 'oil', 'talented', 'become', 'radios', 'sauce', 'gray', 'fruit', 'pollution', 'bumps', 'worry', 'problem', 'calories', 'telling', 'whole', 'switch', 'inviting', 'registration', 'average', 'activities', 'children', 'coworkers', 'describe', 'birds', 'hearing', 'final', 'followed', 'rent', 'bleeding', 'chucks'}\n",
            "2288\n",
            "2344\n",
            "['a', 'able', 'about', 'absolutely', 'accident', 'accidents', 'aches', 'across', 'act', 'action', 'activities', 'actor', 'actors', 'actress', 'actually', 'add', 'added', 'adding', 'address', 'ads']\n",
            "['<END>', '<START>', 'a', 'able', 'about', 'absolutely', 'accident', 'accidents', 'aches', 'act', 'action', 'activities', 'actor', 'actors', 'actress', 'actually', 'add', 'added', 'adding', 'address']\n",
            "3725\n",
            "3725\n",
            "['the forecast says that it will be warm on the weekend ', 'yes  those eight years were a lot of fun for everyone ', 'what happened ', 'yeah  i went  did you go ', 'i hope i win the lotto ', 'a good story is more important than color ', 'then they re worth every penny ', 'me  too  school was fun ', 'i have to go to the bathroom ', 'and you get a lot of exercise every day ', 'are you sure ', 'next time you go to the market  let me go with you ', 'we get a lot of things from cows  don t we ', 'we need cheese  bread  and ham ', 'of course  it s not a hard job ', 'every easter sunday he gives away money ', 'you would do the same for me ', 'it s worse than that ', 'what s the matter ', 'yes  after two u s  fighter jets followed him for an hour  he landed on a highway ']\n",
            "['<START> so do you think it ll be perfect weather for the beach <END>', '<START> only american soldiers were killed overseas <END>', '<START> i gave her for her birthday i told her to spend it on herself <END>', '<START> no i didn t feel like it <END>', '<START> your chances are very small <END>', '<START> actors didn t curse back then <END>', '<START> you might want to buy a pair <END>', '<START> and it was only years <END>', '<START> you drink too much coffee <END>', '<START> that s the truth <END>', '<START> we will be house rich but cash poor <END>', '<START> no thank you all you want to eat are hot dogs and candy bars <END>', '<START> yes a cow is man s best friend <END>', '<START> what kind of cheese <END>', '<START> i ll help you <END>', '<START> is it his money <END>', '<START> of course what are friends for <END>', '<START> how so <END>', '<START> i was on a plane <END>', '<START> did he crash <END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDHMDQdoOzdO",
        "outputId": "602b2c67-fa22-4e96-d40e-161f62541833"
      },
      "source": [
        "for t in reply_tokens:\n",
        "  if t == '<START>':\n",
        "    print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AN5rG3tIGSS",
        "outputId": "9f8a392a-0a95-4f00-c501-75e2db83c95d"
      },
      "source": [
        "temp = \"<START>  tell me what you've been up to .  <END>\"\n",
        "print(re.findall(r\"[\\w'<>]+|[^\\s\\w]\", temp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<START>', 'tell', 'me', 'what', \"you've\", 'been', 'up', 'to', '.', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROKdRzyaNoa0",
        "outputId": "a9347fed-1ee4-43bb-f0d2-4a82f425156b"
      },
      "source": [
        "temp = \"tell me what you've been up to.\"\n",
        "temp1 = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", temp))\n",
        "print(temp1)\n",
        "temp1 = '<START> ' + temp1 + ' <END>'\n",
        "print(temp1)\n",
        "print(temp1.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tell me what you've been up to .\n",
            "<START> tell me what you've been up to . <END>\n",
            "['<START>', 'tell', 'me', 'what', \"you've\", 'been', 'up', 'to', '.', '<END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eejBAAvd1LTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cef1d1-c4b3-4798-a84a-8740d4cf6e03"
      },
      "source": [
        "\"<START> tell me what you've been up to. <END>\".split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START>', 'tell', 'me', 'what', \"you've\", 'been', 'up', 'to.', '<END>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5oH-jCQJUbs"
      },
      "source": [
        "## Use three matrices of one-hot vectors: Encoder input data, Decoder input data, and Decoder output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-O41wmuOJbq",
        "outputId": "a952f668-984c-4566-c028-0ba3ea360d69"
      },
      "source": [
        "[len(re.findall(r\"[\\w'<>]+|[^\\s\\w]\", sens)) for sens in [\"<START> tell me what you've been up to . <END>\",\"I lob\"]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ26htMg0lhP"
      },
      "source": [
        "def get_token_array(sentences_array, decoder_reply=\"\"):\n",
        "  ''' Input:\n",
        "          sentences_array : an array of sentences\n",
        "      Output:\n",
        "          tokens_array : return an array of unique tokens \n",
        "  '''\n",
        "  tokens_array = set()\n",
        "  for sentence in sentences_array:\n",
        "    #if decoder_reply == \"encoder_input\":\n",
        "      #sentence_ = \n",
        "    for token in sentence.split():\n",
        "      tokens_array.add(token)\n",
        "\n",
        "  tokens_array = sorted(list(tokens_array))\n",
        "  return tokens_array\n",
        "\n",
        "def one_hot_matrix(sentences_array, token_array, decoder_reply=\"\"):\n",
        "  ''' Input:\n",
        "          sentences_array : an array of sentences\n",
        "      Output:\n",
        "          text_matrix : return a 3-d one-hot-vector matrix\n",
        "  '''\n",
        "  dim1 = len(sentences_array)\n",
        "\n",
        "  #token_array = get_token_array(sentences_array, decoder_reply)\n",
        "  dim3 = len(token_array)\n",
        "\n",
        "  #Maximum length of sentences in input and target documents\n",
        "  dim2 = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", sens)) for sens in sentences_array])\n",
        "\n",
        "  # create a dictionary of key-values as token-index\n",
        "  features_dict = dict([(token, i) for i, token in enumerate(token_array)])\n",
        "  \n",
        "  #initialize 3d-array\n",
        "  text_matrix = np.zeros((dim1, dim2, dim3), dtype='float32')\n",
        "\n",
        "  for line_index, line in enumerate(sentences_array):\n",
        "    for token_index, token in enumerate(line.split()):\n",
        "      if decoder_reply == \"decoder_target\" and token_index > 0:\n",
        "        text_matrix[line_index , token_index-1, features_dict[token]] = 1\n",
        "      elif decoder_reply != \"decoder_target\":\n",
        "        text_matrix[line_index , token_index, features_dict[token]] = 1\n",
        "  \n",
        "  return text_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz4BYynZStJv"
      },
      "source": [
        "encoder_input_model  = one_hot_matrix(input_lines_2, input_tokens, \"\")\n",
        "decoder_input_model = one_hot_matrix(reply_lines_2, reply_tokens, \"\")\n",
        "decoder_target_model = one_hot_matrix(reply_lines_2, reply_tokens, \"decoder_target\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4tJGHgxquJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c8e252-0176-4412-b202-f3f7413bbad5"
      },
      "source": [
        "encoder_input_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekdsXrG6qdNw"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMjk-zBmGF3f"
      },
      "source": [
        "# Analyis applying Sequence to Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqbqvspJr-7J"
      },
      "source": [
        "**Sequence to sequence** is a method of encoder-decoder based machine translation and language processing that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNNs that will work together with a special token and try to predict the next state sequence from the previous sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5hc01BxYtiG"
      },
      "source": [
        "The **Sequence to sequence** model also called the encoder-decoder model uses Long Short Term Memory- LSTM for text generation from the training corpus.  \n",
        "\n",
        "Goal: It predicts a word given in the user input and then each of the next words is predicted using the probability of likelihood of that word to occur.  \n",
        "\n",
        "The encoder model includes an input layer which defines a matrix for holding the one-hot vectors and an LSTM layer with some number of hidden states.  \n",
        "\n",
        "The encoder outputs a final state vector (memory) which becomes the initial state for the decoder.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=11X90gJrld15icniXmP8hyJH35v4Z2HYD\" width=\"400\" height=\"280\" />\n",
        "\n",
        "A method called **teacher forcing** to train the decoder which enables it to predict the following words in a target sequence given in the previous words. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vRmDM9vordxcTlORtZ1pwWsWRpgNIlhY\" width=\"400\" height=\"280\" />\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rJ0EdaIMFAZBkiAop7K96OYdseHQcURy\" width=\"400\" height=\"280\" />\n",
        "\n",
        "\n",
        "Pros:\n",
        "* Training with Teacher Forcing converges faster. \n",
        "* The model will be updated by a sequence of better predictions.  \n",
        "\n",
        "\n",
        "Cons:\n",
        "* During inference, since there is usually no ground truth available, the RNN model will need to feed its own previous prediction back to itself for the next prediction. Therefore there is a discrepancy between training and inference, and this might lead to poor model performance and instability. This is known as Exposure Bias in literature.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogw9SKYCGKxM"
      },
      "source": [
        "Reference:\n",
        "* what is seq2seq? https://www.guru99.com/seq2seq-model.html#:~:text=Seq2Seq%20is%20a%20method%20of,sequence%20from%20the%20previous%20sequence. \n",
        "* https://github.com/jackfrost1411/Generative-chatbot?fbclid=IwAR2J5Q6caH_0RI4r7o1aUk1Rwv5ytEENORDXiK_RsO30Fg3aKzl65zcqCjo\n",
        "* https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-d411c8738ab5\n",
        "* https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9DyHuy8sX4E"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io02YvJhEnwF"
      },
      "source": [
        "#Dimensionality\n",
        "dimensionality = 256\n",
        "\n",
        "#The batch size and number of epochs\n",
        "batch_size = 10\n",
        "epochs = 60\n",
        "\n",
        "num_encoder_tokens = len(input_tokens) #len(get_token_array(input_lines_2))\n",
        "num_decoder_tokens = len(reply_tokens) #len(get_token_array(reply_lines_2))\n",
        "\n",
        "#Encoder\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
        "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_hidden, state_cell]\n",
        "\n",
        "#Decoder\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hs-evMaEnzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4afbde4-e2b0-4eef-a2e1-0a858db129f0"
      },
      "source": [
        "#Model\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#Compiling\n",
        "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \n",
        "                       metrics=['accuracy'], sample_weight_mode='temporal')\n",
        "\n",
        "#Training\n",
        "training_model.fit([encoder_input_model, decoder_input_model], decoder_target_model, \n",
        "                   batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
        "training_model.save('training_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "298/298 [==============================] - 132s 433ms/step - loss: 1.8253 - accuracy: 0.7236 - val_loss: 1.7451 - val_accuracy: 0.7479\n",
            "Epoch 2/60\n",
            "298/298 [==============================] - 124s 415ms/step - loss: 1.7209 - accuracy: 0.7419 - val_loss: 1.7373 - val_accuracy: 0.7483\n",
            "Epoch 3/60\n",
            "298/298 [==============================] - 124s 416ms/step - loss: 1.7088 - accuracy: 0.7445 - val_loss: 1.7335 - val_accuracy: 0.7469\n",
            "Epoch 4/60\n",
            "298/298 [==============================] - 125s 421ms/step - loss: 1.7172 - accuracy: 0.7424 - val_loss: 1.7330 - val_accuracy: 0.7486\n",
            "Epoch 5/60\n",
            "298/298 [==============================] - 135s 453ms/step - loss: 1.6987 - accuracy: 0.7456 - val_loss: 1.7297 - val_accuracy: 0.7484\n",
            "Epoch 6/60\n",
            "298/298 [==============================] - 136s 457ms/step - loss: 1.6800 - accuracy: 0.7479 - val_loss: 1.7210 - val_accuracy: 0.7505\n",
            "Epoch 7/60\n",
            "298/298 [==============================] - 133s 447ms/step - loss: 1.6657 - accuracy: 0.7502 - val_loss: 1.7209 - val_accuracy: 0.7499\n",
            "Epoch 8/60\n",
            "298/298 [==============================] - 131s 441ms/step - loss: 1.6920 - accuracy: 0.7461 - val_loss: 1.7119 - val_accuracy: 0.7493\n",
            "Epoch 9/60\n",
            "298/298 [==============================] - 133s 447ms/step - loss: 1.6719 - accuracy: 0.7478 - val_loss: 1.7106 - val_accuracy: 0.7528\n",
            "Epoch 10/60\n",
            "298/298 [==============================] - 132s 443ms/step - loss: 1.6703 - accuracy: 0.7478 - val_loss: 1.7126 - val_accuracy: 0.7530\n",
            "Epoch 11/60\n",
            "298/298 [==============================] - 132s 443ms/step - loss: 1.6495 - accuracy: 0.7517 - val_loss: 1.7108 - val_accuracy: 0.7538\n",
            "Epoch 12/60\n",
            "298/298 [==============================] - 132s 442ms/step - loss: 1.6671 - accuracy: 0.7490 - val_loss: 1.7142 - val_accuracy: 0.7525\n",
            "Epoch 13/60\n",
            "298/298 [==============================] - 131s 439ms/step - loss: 1.6548 - accuracy: 0.7505 - val_loss: 1.7059 - val_accuracy: 0.7544\n",
            "Epoch 14/60\n",
            "298/298 [==============================] - 129s 434ms/step - loss: 1.6219 - accuracy: 0.7558 - val_loss: 1.7007 - val_accuracy: 0.7532\n",
            "Epoch 15/60\n",
            "298/298 [==============================] - 128s 430ms/step - loss: 1.6294 - accuracy: 0.7539 - val_loss: 1.6994 - val_accuracy: 0.7538\n",
            "Epoch 16/60\n",
            "298/298 [==============================] - 128s 430ms/step - loss: 1.5930 - accuracy: 0.7587 - val_loss: 1.7026 - val_accuracy: 0.7534\n",
            "Epoch 17/60\n",
            "298/298 [==============================] - 129s 432ms/step - loss: 1.6256 - accuracy: 0.7516 - val_loss: 1.7097 - val_accuracy: 0.7542\n",
            "Epoch 18/60\n",
            "298/298 [==============================] - 129s 431ms/step - loss: 1.6175 - accuracy: 0.7548 - val_loss: 1.6948 - val_accuracy: 0.7557\n",
            "Epoch 19/60\n",
            "298/298 [==============================] - 127s 426ms/step - loss: 1.6222 - accuracy: 0.7533 - val_loss: 1.6922 - val_accuracy: 0.7552\n",
            "Epoch 20/60\n",
            "298/298 [==============================] - 127s 427ms/step - loss: 1.5932 - accuracy: 0.7579 - val_loss: 1.6963 - val_accuracy: 0.7547\n",
            "Epoch 21/60\n",
            "298/298 [==============================] - 129s 433ms/step - loss: 1.5954 - accuracy: 0.7566 - val_loss: 1.6940 - val_accuracy: 0.7546\n",
            "Epoch 22/60\n",
            "298/298 [==============================] - 131s 440ms/step - loss: 1.5775 - accuracy: 0.7586 - val_loss: 1.6841 - val_accuracy: 0.7558\n",
            "Epoch 23/60\n",
            "298/298 [==============================] - 130s 437ms/step - loss: 1.6065 - accuracy: 0.7542 - val_loss: 1.6896 - val_accuracy: 0.7558\n",
            "Epoch 24/60\n",
            "298/298 [==============================] - 130s 437ms/step - loss: 1.5821 - accuracy: 0.7575 - val_loss: 1.6888 - val_accuracy: 0.7557\n",
            "Epoch 25/60\n",
            "298/298 [==============================] - 129s 433ms/step - loss: 1.5771 - accuracy: 0.7588 - val_loss: 1.6899 - val_accuracy: 0.7558\n",
            "Epoch 26/60\n",
            "298/298 [==============================] - 128s 430ms/step - loss: 1.5546 - accuracy: 0.7605 - val_loss: 1.6915 - val_accuracy: 0.7566\n",
            "Epoch 27/60\n",
            "298/298 [==============================] - 126s 424ms/step - loss: 1.5859 - accuracy: 0.7554 - val_loss: 1.6908 - val_accuracy: 0.7557\n",
            "Epoch 28/60\n",
            "298/298 [==============================] - 126s 422ms/step - loss: 1.5603 - accuracy: 0.7599 - val_loss: 1.6867 - val_accuracy: 0.7554\n",
            "Epoch 29/60\n",
            "298/298 [==============================] - 126s 424ms/step - loss: 1.5462 - accuracy: 0.7605 - val_loss: 1.6966 - val_accuracy: 0.7562\n",
            "Epoch 30/60\n",
            "298/298 [==============================] - 128s 430ms/step - loss: 1.5480 - accuracy: 0.7601 - val_loss: 1.6889 - val_accuracy: 0.7557\n",
            "Epoch 31/60\n",
            "298/298 [==============================] - 127s 425ms/step - loss: 1.5277 - accuracy: 0.7634 - val_loss: 1.6988 - val_accuracy: 0.7558\n",
            "Epoch 32/60\n",
            "298/298 [==============================] - 129s 433ms/step - loss: 1.5384 - accuracy: 0.7613 - val_loss: 1.7023 - val_accuracy: 0.7538\n",
            "Epoch 33/60\n",
            "298/298 [==============================] - 132s 444ms/step - loss: 1.5207 - accuracy: 0.7643 - val_loss: 1.7032 - val_accuracy: 0.7548\n",
            "Epoch 34/60\n",
            "298/298 [==============================] - 132s 443ms/step - loss: 1.5002 - accuracy: 0.7668 - val_loss: 1.7042 - val_accuracy: 0.7558\n",
            "Epoch 35/60\n",
            "298/298 [==============================] - 133s 446ms/step - loss: 1.5114 - accuracy: 0.7654 - val_loss: 1.7097 - val_accuracy: 0.7550\n",
            "Epoch 36/60\n",
            "298/298 [==============================] - 132s 442ms/step - loss: 1.4948 - accuracy: 0.7688 - val_loss: 1.7105 - val_accuracy: 0.7561\n",
            "Epoch 37/60\n",
            "298/298 [==============================] - 129s 432ms/step - loss: 1.5058 - accuracy: 0.7648 - val_loss: 1.7154 - val_accuracy: 0.7562\n",
            "Epoch 38/60\n",
            "298/298 [==============================] - 129s 432ms/step - loss: 1.4678 - accuracy: 0.7716 - val_loss: 1.7229 - val_accuracy: 0.7552\n",
            "Epoch 39/60\n",
            "298/298 [==============================] - 128s 429ms/step - loss: 1.4793 - accuracy: 0.7685 - val_loss: 1.7225 - val_accuracy: 0.7542\n",
            "Epoch 40/60\n",
            "298/298 [==============================] - 132s 444ms/step - loss: 1.4679 - accuracy: 0.7697 - val_loss: 1.7262 - val_accuracy: 0.7550\n",
            "Epoch 41/60\n",
            "298/298 [==============================] - 132s 442ms/step - loss: 1.4767 - accuracy: 0.7696 - val_loss: 1.7322 - val_accuracy: 0.7555\n",
            "Epoch 42/60\n",
            "298/298 [==============================] - 131s 440ms/step - loss: 1.4435 - accuracy: 0.7753 - val_loss: 1.7370 - val_accuracy: 0.7544\n",
            "Epoch 43/60\n",
            "298/298 [==============================] - 131s 440ms/step - loss: 1.4405 - accuracy: 0.7749 - val_loss: 1.7432 - val_accuracy: 0.7549\n",
            "Epoch 44/60\n",
            "298/298 [==============================] - 134s 451ms/step - loss: 1.4499 - accuracy: 0.7721 - val_loss: 1.7604 - val_accuracy: 0.7544\n",
            "Epoch 45/60\n",
            "298/298 [==============================] - 133s 447ms/step - loss: 1.4213 - accuracy: 0.7791 - val_loss: 1.7398 - val_accuracy: 0.7548\n",
            "Epoch 46/60\n",
            "298/298 [==============================] - 133s 448ms/step - loss: 1.4305 - accuracy: 0.7759 - val_loss: 1.7518 - val_accuracy: 0.7545\n",
            "Epoch 47/60\n",
            "298/298 [==============================] - 132s 443ms/step - loss: 1.4225 - accuracy: 0.7777 - val_loss: 1.7696 - val_accuracy: 0.7546\n",
            "Epoch 48/60\n",
            "298/298 [==============================] - 123s 411ms/step - loss: 1.3955 - accuracy: 0.7814 - val_loss: 1.7609 - val_accuracy: 0.7538\n",
            "Epoch 49/60\n",
            "298/298 [==============================] - 127s 427ms/step - loss: 1.3931 - accuracy: 0.7808 - val_loss: 1.7680 - val_accuracy: 0.7532\n",
            "Epoch 50/60\n",
            "298/298 [==============================] - 126s 423ms/step - loss: 1.3820 - accuracy: 0.7825 - val_loss: 1.7858 - val_accuracy: 0.7535\n",
            "Epoch 51/60\n",
            "298/298 [==============================] - 125s 420ms/step - loss: 1.3918 - accuracy: 0.7829 - val_loss: 1.7883 - val_accuracy: 0.7557\n",
            "Epoch 52/60\n",
            "298/298 [==============================] - 125s 420ms/step - loss: 1.3680 - accuracy: 0.7850 - val_loss: 1.7990 - val_accuracy: 0.7533\n",
            "Epoch 53/60\n",
            "298/298 [==============================] - 124s 416ms/step - loss: 1.3508 - accuracy: 0.7885 - val_loss: 1.7982 - val_accuracy: 0.7523\n",
            "Epoch 54/60\n",
            "298/298 [==============================] - 124s 416ms/step - loss: 1.3437 - accuracy: 0.7887 - val_loss: 1.8033 - val_accuracy: 0.7538\n",
            "Epoch 55/60\n",
            "298/298 [==============================] - 124s 417ms/step - loss: 1.3291 - accuracy: 0.7914 - val_loss: 1.8020 - val_accuracy: 0.7542\n",
            "Epoch 56/60\n",
            "298/298 [==============================] - 124s 416ms/step - loss: 1.3114 - accuracy: 0.7948 - val_loss: 1.8287 - val_accuracy: 0.7534\n",
            "Epoch 57/60\n",
            "298/298 [==============================] - 123s 414ms/step - loss: 1.3343 - accuracy: 0.7912 - val_loss: 1.8244 - val_accuracy: 0.7527\n",
            "Epoch 58/60\n",
            "298/298 [==============================] - 124s 417ms/step - loss: 1.3391 - accuracy: 0.7908 - val_loss: 1.8430 - val_accuracy: 0.7532\n",
            "Epoch 59/60\n",
            "298/298 [==============================] - 123s 412ms/step - loss: 1.3065 - accuracy: 0.7957 - val_loss: 1.8301 - val_accuracy: 0.7533\n",
            "Epoch 60/60\n",
            "298/298 [==============================] - 122s 410ms/step - loss: 1.2990 - accuracy: 0.7986 - val_loss: 1.8548 - val_accuracy: 0.7537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkSfgUsqsc-I"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzlno7cd_XJU"
      },
      "source": [
        "training_model = load_model('training_model.h5')\n",
        "\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFPNCvA_YfL"
      },
      "source": [
        "latent_dim = 256\n",
        "\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UusZz-GNOqI9"
      },
      "source": [
        "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_r3O6B_bK0"
      },
      "source": [
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZro0npA7y2d"
      },
      "source": [
        "'''token_array = get_token_array(reply_lines_2)\n",
        "target_features_dict = dict([(token, i) for i, token in enumerate(token_array)])\n",
        "i=0\n",
        "for key,value in target_features_dict.items():\n",
        "  if i == 30:\n",
        "    break \n",
        "  i += 1\n",
        "  print (key, \" \", value)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKL_ZAzJs5y1"
      },
      "source": [
        "def decode_response(test_input):\n",
        "  token_array = reply_tokens #get_token_array(reply_lines_2)\n",
        "  target_features_dict = dict([(token, i) for i, token in enumerate(token_array)])\n",
        "  reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())\n",
        "  max_decoder_seq_length = max([len(sens.split()) for sens in reply_lines_2])\n",
        "  \n",
        "  #Getting the output states to pass into the decoder\n",
        "  states_value = encoder_model.predict(test_input)\n",
        "  #Generating empty target sequence of length 1\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  #Setting the first token of target sequence with the start token\n",
        "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "    \n",
        "  #A variable to store our response word by word\n",
        "  decoded_sentence = ''\n",
        "    \n",
        "  stop_condition = False\n",
        "  while not stop_condition:\n",
        "    #Predicting output tokens with probabilities and states\n",
        "    output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "    #Choosing the one with highest probability\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "    #Stop if hit max length or found the stop token\n",
        "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "\n",
        "    #Update the target sequence\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    #Update states\n",
        "    states_value = [hidden_state, cell_state]\n",
        "\n",
        "  return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bj6G0y5xgWw"
      },
      "source": [
        "# Generate Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNXlSRIkR0x1"
      },
      "source": [
        "#token_array = get_token_array(user_input)\n",
        "#input_features_dict =  dict([(token, i) for i, token in enumerate(token_array)])\n",
        "#max_encoder_seq_length = max([len(sens.split()) for sens in user_input])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdOn5r34Ckty"
      },
      "source": [
        "class ChatBot:\n",
        "  negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
        "  \n",
        "  #Method to start the conversation\n",
        "  def start_chat(self):\n",
        "    user_response = input(\"Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\\n\")\n",
        "    \n",
        "    if user_response in self.negative_responses:\n",
        "      print(\"Ok, have a great day!\")\n",
        "      return\n",
        "    self.chat(user_response)\n",
        "\n",
        "  #Method to handle the conversation\n",
        "  def chat(self, reply):\n",
        "    while not self.make_exit(reply):\n",
        "      reply = input(self.generate_response(reply)+\"\\n\")\n",
        "    \n",
        "  #Method to convert user input into a matrix\n",
        "  def string_to_matrix(self, user_input):\n",
        "    token_array = reply_tokens #get_token_array(input_lines_2)\n",
        "    input_features_dict =  dict([(token, i) for i, token in enumerate(token_array)])\n",
        "    max_encoder_seq_length = max([len(sens.split()) for sens in input_lines_2])\n",
        "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
        "    user_input_matrix = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "    \n",
        "    for timestep, token in enumerate(tokens):\n",
        "      if token in input_features_dict:\n",
        "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
        "    return user_input_matrix\n",
        "  \n",
        "  #Method that will create a response using seq2seq model we built\n",
        "  def generate_response(self, user_input):\n",
        "    input_matrix = self.string_to_matrix(user_input)\n",
        "    chatbot_response = decode_response(input_matrix)\n",
        "    #Remove <START> and <END> tokens from chatbot_response\n",
        "    chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
        "    return chatbot_response\n",
        "  #Method to check for exit commands\n",
        "  def make_exit(self, reply):\n",
        "    for exit_command in self.exit_commands:\n",
        "      if exit_command in reply:\n",
        "        print(\"Ok, have a great day!\")\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "chatbot = ChatBot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnoOPDbQxisA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80faf53-419d-481c-8207-6511e5b4cd3a"
      },
      "source": [
        "chatbot.start_chat() # for 10 epoches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
            "hi there\n",
            " what happened \n",
            "nothing\n",
            " what happened \n",
            "nah\n",
            " what happened \n",
            "tired\n",
            " what happened \n",
            "good\n",
            " stress \n",
            "really?\n",
            " yes i \n",
            "ok\n",
            " what happened \n",
            "bye\n",
            "Ok, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niuROuJ9xivr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68da2f8-8b31-44f6-aa8e-ac737004ca04"
      },
      "source": [
        "chatbot.start_chat() # for 20 epoches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
            "hi there\n",
            " what happened \n",
            "nothing\n",
            " what happened \n",
            "tired\n",
            " i agree \n",
            "what\n",
            " what happened \n",
            "nothing\n",
            " what happened \n",
            "nope\n",
            " i agree \n",
            "ok\n",
            " i agree \n",
            "vacation\n",
            " i agree \n",
            "i am tired\n",
            " i was \n",
            "really\n",
            " teaching the is \n",
            "ok\n",
            " i agree \n",
            "bye\n",
            "Ok, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gRqHvuAX7Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd91414-47b0-45d3-8561-05a5c3098bc6"
      },
      "source": [
        "chatbot.start_chat() # for 30 epoches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
            "hi there\n",
            " i m t \n",
            "good\n",
            " me causes \n",
            "ok\n",
            " thank you \n",
            "no problem\n",
            " thank you \n",
            "byt\n",
            " thank you \n",
            "bye\n",
            "Ok, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0j5e9Zn6wtg",
        "outputId": "b0084d62-330b-454e-9431-085da46548af"
      },
      "source": [
        "chatbot.start_chat() # for 60 epoches"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
            "hi there\n",
            " thank you \n",
            "=)\n",
            " well neither \n",
            "cool\n",
            " i like a \n",
            "me too\n",
            " i agree \n",
            "awesome\n",
            " oh yes \n",
            "talk to you later\n",
            "Ok, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D9Qj7kUa-nL",
        "outputId": "9b114b91-3448-40cf-f92f-4a53fe2ae500"
      },
      "source": [
        "chatbot.start_chat() # this is the one on the poster"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi, I'm a chatbot trained on random dialogs. Would you like to chat with me?\n",
            "hi there\n",
            " thank you \n",
            "=)\n",
            " well neither \n",
            "cool\n",
            " i like a \n",
            "awesome\n",
            " oh yes \n",
            "see you later\n",
            "Ok, have a great day!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbzJh6S4trys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}